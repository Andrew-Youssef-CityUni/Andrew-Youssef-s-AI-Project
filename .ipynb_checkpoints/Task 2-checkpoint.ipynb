{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (4, 4)\n",
      "AL = [[0.5 0.5 0.5 0.5]]\n",
      "Length of pass_variable list =  9\n",
      "parameters: {'W1': array([[ 0.01132424, -0.00831164,  0.00245591,  0.01590985],\n",
      "       [-0.00692956, -0.00052132,  0.01127494, -0.01044558],\n",
      "       [-0.00389963, -0.01496864,  0.00289895,  0.02719597]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.02006203, -0.02200051, -0.0081802 ],\n",
      "       [-0.00288442,  0.01046203, -0.00958938]]), 'b2': array([[0.],\n",
      "       [0.]]), 'W3': array([[ 0.00926302, -0.01061401],\n",
      "       [ 0.00788708,  0.00640199]]), 'b3': array([[0.],\n",
      "       [0.]]), 'W4': array([[-0.00109395,  0.00150953]]), 'b4': array([[0.]])}\n",
      "cost =  0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    pass_variable = Z\n",
    "    return A, pass_variable\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)  \n",
    "    pass_variable = Z \n",
    "    return A, pass_variable\n",
    "                                          \n",
    "def sigmoid_backward(dA, pass_variable):\n",
    "    Z = pass_variable \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, pass_variable):\n",
    "    Z = pass_variable\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def initialize_Layer(input_layer, hidden_layer, output_layer):\n",
    "    # initialize 1st layer output and input with random values\n",
    "    W1 = np.random.randn(hidden_layer, input_layer) * 0.01\n",
    "    # initialize 1st layer output bias\n",
    "    b1 = np.zeros((hidden_layer, 1))\n",
    "    # initialize 2nd layer output and input with random values\n",
    "    W2 = np.random.randn(output_layer, hidden_layer) * 0.01\n",
    "    # initialize 2nd layer output bias\n",
    "    b2 = np.zeros((output_layer,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def initialize_parameters_deep(layer_dimension):\n",
    "    parameters = {}\n",
    "\n",
    "    L = len(layer_dimension)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dimension[l], layer_dimension[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dimension[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def feedforward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "\n",
    "    pass_variable = (A, W, b)\n",
    "    \n",
    "    return Z, pass_variable\n",
    "\n",
    "def activation_feedforward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":     \n",
    "        Z, linear_pass_variable = feedforward(A_prev,W,b)\n",
    "        A, activation_pass_variable = sigmoid(Z)      \n",
    "    \n",
    "    elif activation == \"relu\":     \n",
    "        Z, linear_pass_variable  = feedforward(A_prev,W,b)\n",
    "        A, activation_pass_variable = relu(Z)\n",
    "    \n",
    "  #   A, activation_pass_variable = softmax(Z)\n",
    "    pass_variable = (linear_pass_variable , activation_pass_variable)\n",
    "    \n",
    "    return A, pass_variable\n",
    "\n",
    "def model_forward(X, parameters):\n",
    "    pass_variables = []\n",
    "    A = X\n",
    "\n",
    "    # number of layers in the neural network\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, pass_variable = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        pass_variables.append(pass_variable)\n",
    "   \n",
    "    AL, pass_variable = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "\n",
    "    pass_variables.append(pass_variable)\n",
    "            \n",
    "    return AL, pass_variables\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # Compute loss from AL and y.\n",
    "    cost = -np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))/m\n",
    "    cost = np.squeeze(cost)  \n",
    "    return cost\n",
    "\n",
    "def backwardpass(dZ, pass_variable):\n",
    "    A_prev, W, b = pass_variable\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def activation_backwardpass(dA, pass_variable, activation):\n",
    "\n",
    "    linear_pass_variable, activation_pass_variable = pass_variable\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, pass_variable[1])\n",
    "        dA_prev, dW, db = backwardpass(dZ, pass_variable[0])\n",
    "   \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, pass_variable[1])\n",
    "        dA_prev, dW, db = backwardpass(dZ, pass_variable[0])\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def model_backwardpass(AL, Y, pass_variables):\n",
    "    grads = {}\n",
    "    # the number of layers\n",
    "    L = len(pass_variables)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_pass_variable = pass_variables[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_pass_variable, \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "\n",
    "        current_pass_variable = pass_variables[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backwardpass(grads[\"dA\"+str(l+1)], current_pass_variable, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    # number of layers in the neural network\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def layer_model(X, Y, layers_dims, learning_rate = 0.01, num_iterations = 100, print_cost=False):\n",
    "    \n",
    "    # keep track of cost\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        AL, pass_variables = model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = model_backwardpass(AL, Y, pass_variables)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "                                          \n",
    "layer_dims = [4,3,2,2,1]\n",
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "X = np.random.rand(4, 4)\n",
    "Y = np.array([[1, 1, 0, 0]])\n",
    "AL, pass_variables = model_forward(X, parameters)\n",
    "\n",
    "print(\"X.shape =\", X.shape)\n",
    "print(\"AL =\", AL)\n",
    "print(\"Length of pass_variable list = \", len(caches))\n",
    "print(\"parameters:\", parameters)\n",
    "print(\"cost = \", compute_cost(AL, Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(\"Training label shape: \", y_train.shape)\n",
    "print(\"First 5 training labels: \", y_train[:5])\n",
    "\n",
    "image_vector_size = 28*28\n",
    "x_train = torch.from_numpy(x_train.reshape(x_train.shape[0], image_vector_size))\n",
    "x_test = torch.from_numpy(x_test.reshape(x_test.shape[0], image_vector_size ))\n",
    "y_train = y_train\n",
    "y_test = y_test\n",
    "\n",
    "class Network(object): \n",
    "    \n",
    "    def _init_(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "           \"\"\"a = Relu(np.dot(w, a)+b)\"\"\" \n",
    "        output = softmax(np.dot(w, a)+b)\n",
    "        return output\n",
    "                                   \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):  \n",
    "                                   \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "                                   \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            \"\"\"activation = relu(z)\"\"\"\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        \"\"\"  delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            softmax_prime(zs[-1])\"\"\"\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        \"\"\"delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            relu_prime(zs[-1])\"\"\"\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            \"\"\"delta = np.dot(self.weights[-l+1].transpose(), delta) * relu_prime(z)\"\"\"\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "        for (x, y) in test_data]\n",
    "            return sum(int(x == y) for (x, y) in test_results)\n",
    "        \n",
    "    def cost_derivative(self, output_activations, y):   \n",
    "            return (output_activations-y)\n",
    "        \n",
    "    def sigmoid(z):\n",
    "    \n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(z):\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "    def ReLU(z):\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def ReLU_prime(z):\n",
    "        return 1. * (z > 0)\n",
    "    \n",
    "    def softmax(z):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "    \n",
    "    def softmax_prime(z):\n",
    "        exps = np.exp(x - x.max())\n",
    "        return  exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0)\n",
    "                                       \n",
    "net = network.Network([784, 30, 10])\n",
    "net.SGD(x_train, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1e752760f240>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-1e752760f240>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    train_X, train_Y, test_X, test_Y =  mnist.load_data()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Extra code not needed\"\"\"\"\n",
    "def softmax(z):\n",
    "    # Numerically stable with large exponentials\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "def softmax_backward(z):\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "train_X, train_Y, test_X, test_Y =  mnist.load_data()\n",
    "image_vector_size = 28*28                                          \n",
    "x_train = torch.from_numpy(x_train.reshape(x_train.shape[0], image_vector_size))\n",
    "x_test = torch.from_numpy(x_test.reshape(x_test.shape[0], image_vector_size ))\n",
    "                                          \n",
    "layers_dims = [2, 4, 1]\n",
    "parameters = L_layer_model(x_train, y_train, layers_dims, learning_rate = 0.2, num_iterations = 15000, print_cost = True)\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(predict(x_train, parameters) -  y_train)) * 100))\n",
    "plot_decision_boundary(lambda x: predict(x.T, parameters), x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
