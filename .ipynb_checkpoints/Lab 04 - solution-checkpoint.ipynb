{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04\n",
    "\n",
    "In this lab, we will first create a dataset for Linear regression, then build different methods to solve linear regression in 3D.\n",
    "\n",
    "We will go from generating synthetic data to optimizing the parameters of our model and plotting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Build a class to generate 3D lines given line parameters, and noise levels:\n",
    "- the noise Eps is Normal centered on 0, with a parameterized variance.\n",
    "- all the generated points are 3D, and in the \\[-10 , 10\\] cube.\n",
    "\n",
    "All points follow the equation:\n",
    "\n",
    "y = a x_0 + b x_1 + c + Eps\n",
    "\n",
    "This class should allow you to generate any number of points.\n",
    "\n",
    "Build a class method that allow you to generate training,  and testing datasets:\n",
    "- the method takes as input the total size of the datast, and the ratio of points used to train (between 0 and 1).\n",
    "\n",
    "Implement a method that allows you to split the training set into minibatches.\n",
    "This method will return lists of successive minibatches of data.\n",
    "Each minibatch will have the same size, and datapoints which are not assigned to the last minibatch will be discarded.\n",
    "\n",
    "Finally, implement a method that allows to display points, as well as :\n",
    "- the actual line that was used to generate the points\n",
    "- an optional additional line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We will use the followig command to allow visualization of 3D plots\n",
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "class Line3D:\n",
    "    \n",
    "    def __init__( self, line_parameters = (2, 4, -1), noise_variance = 1 ):\n",
    "        \n",
    "        self.line_parameters = line_parameters\n",
    "        self.noise_variance = noise_variance\n",
    "        \n",
    "    def generate_points(self, N):\n",
    "\n",
    "        # Adding the X_0 coordinates full of ones cost us nothing, but will allow us to use vector notation and calculation. \n",
    "        X_0 = np.ones(N)\n",
    "        X_1 = rng.uniform(-10, 10, N)\n",
    "        X_2 = rng.uniform(-10, 10, N)\n",
    "        \n",
    "        epsilon = rng.normal(0, self.noise_variance**(1/2), N)\n",
    "         \n",
    "        theta_0, theta_1, theta_2 = self.line_parameters\n",
    "        \n",
    "        y = theta_0 + X_1 * theta_1 + X_2 * theta_2 + epsilon\n",
    "        \n",
    "        # We create a design matrix, where rows are datapoints and columns are features, or input dimensions\n",
    "        X = np.vstack( [X_0, X_1, X_2]).transpose()\n",
    "                \n",
    "        return X, y\n",
    "        \n",
    "    def generate_dataset(self, N, ratio_train = 0.6):\n",
    "        \n",
    "        self.X_train , self.y_train = self.generate_points( int(N*ratio_train) )\n",
    "        self.X_test , self.y_test = self.generate_points( N - int(N*ratio_train) )\n",
    "\n",
    "    def get_minibatches_train(self, M):\n",
    "            \n",
    "        size_split = int(len(self.X_train)/M)\n",
    "        \n",
    "        X_minibatch = []\n",
    "        y_minibatch = []\n",
    "        \n",
    "        for i in range(M):\n",
    "            \n",
    "            X_mini = self.X_train[ i * size_split : (i+1)*size_split, : ]\n",
    "            y_mini = self.y_train[ i * size_split : (i+1)*size_split ]\n",
    "            \n",
    "            X_minibatch.append(X_mini)\n",
    "            y_minibatch.append(y_mini)\n",
    "            \n",
    "        return X_minibatch, y_minibatch\n",
    "    \n",
    "    def plot( self, X, y, additional_line_parameters = None ):\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        ax.scatter(X[:, 1], X[:, 2], y, marker = '.')\n",
    "        \n",
    "        X_1 = np.linspace(-10, 10, 2)\n",
    "        X_2 = np.linspace(-10, 10, 2)\n",
    "\n",
    "        X_1, X_2 = np.meshgrid(X_1, X_2)\n",
    "        \n",
    "        if additional_line_parameters is not None:\n",
    "            theta_0, theta_1, theta_2 = additional_line_parameters\n",
    "        else:\n",
    "            theta_0, theta_1, theta_2 = self.line_parameters\n",
    "        \n",
    "        y = theta_0 + X_1 * theta_1 + X_2 * theta_2\n",
    "    \n",
    "        ax.plot_surface(X_1, X_2, y, alpha = 0.4, color='g')\n",
    "        \n",
    "\n",
    "data = Line3D( line_parameters=(1,2,3), noise_variance=100 )\n",
    "X_vis, y_vis = data.generate_points(1000)\n",
    "data.plot(X_vis, y_vis)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens here? Why do we have a plan instead of a line? \n",
    "\n",
    "So, this was confusing on purpose. In 2D, a Linear Regression deals with fitting a model to a line.\n",
    "\n",
    "However, in 3D, the general linear equation y = a * x_1 + b * x_2 + c doesn't correspond to a line, but to a plan.\n",
    "\n",
    "More generally, the linear equation y = sum( a_i * x_i ) corresponds to a hyperplan of dimension N-1.\n",
    "\n",
    "So remember, that talking about linear model doesn't always mean that we are dealing with a line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Now, we will solve Linear Regression using the different methods we saw in class.\n",
    "\n",
    "First of all, we will build an Abstract Base Class that will define how all Linear Regression subclasses behave.\n",
    "Some methods are common to all subclasses that we will implement.\n",
    "\n",
    "We will evaluate all our methods using the r2 score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class LinearRegression(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.plan_params = None\n",
    "        \n",
    "    def cost(self, X, y):\n",
    "        \n",
    "        # Note that another valid formulation of the cost is the Mean Squared Error, where the sum of squared error is normalized by the number of samples.\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        calculated_cost = np.sum( (y-y_pred)**2 )\n",
    "        \n",
    "        return calculated_cost\n",
    "        \n",
    "        \n",
    "    # This will need to be implemented by any Class inheriting from LinearRegression    \n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        # each subclass will reimplement this\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        theta_0, theta_1, theta_2 = self.plan_params\n",
    "        \n",
    "        # We can use vector notation eqivalent to :\n",
    "        # y_pred = X[:,0] * theta_0 + X[:,1] * theta_1 + X[:,2] * theta_2\n",
    "        y_pred = X.dot(self.plan_params)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \n",
    "        y_mean = np.mean(y_test)\n",
    "        \n",
    "        y_pred = self.predict(X_test)\n",
    "        \n",
    "        # Total Sum of Squares\n",
    "        SS_tot = np.sum( (y_test - y_mean)**2 )\n",
    "        \n",
    "        # Residal Sum of Squares\n",
    "        SS_res = np.sum( (y_test - y_pred)**2 )\n",
    "        \n",
    "        # Coefficient of determination\n",
    "        r2 = 1 - SS_res/SS_tot\n",
    "        \n",
    "        return r2\n",
    "    \n",
    "    def plot(self, X, y):\n",
    "        \n",
    "        # plot the points with the plane defined by the current parameters.\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(X[:, 1], X[:, 2], y, marker = '.')\n",
    "        \n",
    "        X_1 = np.linspace(-10, 10, 2)\n",
    "        X_2 = np.linspace(-10, 10, 2)\n",
    "\n",
    "        X_1, X_2 = np.meshgrid(X_1, X_2)\n",
    "        \n",
    "        theta_0, theta_1, theta_2 = self.plan_params\n",
    "        \n",
    "        y = theta_0 + X_1 * theta_1 + X_2 * theta_2\n",
    "    \n",
    "        ax.plot_surface(X_1, X_2, y, alpha = 0.4, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Base class is defined, we can create different subclasses that implement the appraoches we saw during the Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88221872 2.00774937 2.99561708]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "class OrdinaryLeastSquare(LinearRegression):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.plan_params = inv(X.transpose().dot(X) ).dot(X.transpose()).dot(y)\n",
    "        \n",
    "\n",
    "X_train, y_train = data.generate_points(10000)\n",
    "\n",
    "ols = OrdinaryLeastSquare()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "print(ols.plan_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that that we are not too far off. \n",
    "\n",
    "We can check the r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235496864833776\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = data.generate_points(1000)\n",
    "r2 = ols.evaluate(X_test, y_test)\n",
    "\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GradientDescent(LinearRegression):\n",
    "    \n",
    "    def __init__(self, learning_rate):\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.plan_params = rng.normal(size = 3)\n",
    "        \n",
    "    def mse(self, X, y):\n",
    "        \n",
    "        return self.cost(X,y)/len(y)\n",
    "    \n",
    "    def fit(self, X_train, y_train, number_epochs ):\n",
    "        \n",
    "        \n",
    "        cost_history = []\n",
    "        \n",
    "        for epoch in range(number_epochs):\n",
    "            \n",
    "            # we can use vector calculation to compute the value of the gradient\n",
    "            gradient = -(y_train - self.predict(X_train)).transpose().dot(X_train)/len(X_train)\n",
    "            \n",
    "            self.plan_params -= self.lr * gradient  \n",
    "            \n",
    "            cost = self.cost(X_train, y_train)\n",
    "            \n",
    "            cost_history.append(cost)\n",
    "            \n",
    "        return cost_history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68106055 1.99962001 3.02096707]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = data.generate_points(10000)\n",
    "\n",
    "gd = GradientDescent(learning_rate=0.01)\n",
    "cost_hist = gd.fit(X_train, y_train, 100)\n",
    "\n",
    "plt.plot(cost_hist)\n",
    "print(gd.plan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.04554411 2.01849998 3.00176249]\n"
     ]
    }
   ],
   "source": [
    "class StochasticGradientDescent(GradientDescent):\n",
    "    \n",
    "    def fit(self, minibatches, number_epochs):\n",
    "        \n",
    "        # We can use the fact that we already implemented the update rule!\n",
    "        \n",
    "        cost_history_epochs = []\n",
    "        cost_history_updates = []\n",
    "                \n",
    "        for epoch in range(number_epochs):\n",
    "            \n",
    "            for X_train_mini, y_train_mini in zip(minibatches[0], minibatches[1]):\n",
    "                \n",
    "                cost_history_minibatch = super().fit(X_train_mini, y_train_mini, number_epochs = 1)\n",
    "                \n",
    "                # There is only one element in cost_history_minibatch, as we only updated for one epoch\n",
    "                cost_history_updates.append( cost_history_minibatch[0] )\n",
    "                \n",
    "            cost_history_epochs.append(cost_history_updates[-1])\n",
    "            \n",
    "        return cost_history_epochs, cost_history_updates\n",
    "\n",
    "                \n",
    "data.generate_dataset( 10000, ratio_train = 0.8)\n",
    "        \n",
    "minibatches = data.get_minibatches_train( 60 )\n",
    "sgd = StochasticGradientDescent(learning_rate=0.001)\n",
    "cost_history_epochs, cost_history_updates = sgd.fit(minibatches, 100)\n",
    "\n",
    "# plt.plot(cost_history_epochs)\n",
    "plt.plot(cost_history_updates[:1000])\n",
    "\n",
    "print(sgd.plan_params)           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a certain periodicity in the Loss, why is that?\n",
    "\n",
    "it has to do with the way we built the minibatches!\n",
    "In principle, it is always a good idea to shuffle the minibatch after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Solve the linear regression problem for different values of line parameters.\n",
    "\n",
    "Compare the speed of convergence of different approaches depending on the learning rate.\n",
    "\n",
    "What happens if the learning rate is too high?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Use scikit-learn to verify that you obtain the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
